{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prepro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMg8b+Dgy5xTs0+c+tbobFk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parthpanchal2402/Coursera_Capstone/blob/master/prepro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFC6lwhRUx_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqAKehd2Un4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import gc\n",
        "\n",
        "from sklearn.exceptions import NotFittedError\n",
        "\n",
        "from itertools import chain"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDmtwiLeU1k8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "data = pd.read_json(r'/content/drive/My Drive/transactions.json', lines=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAnrhhrvpLAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEB2JWB2__n3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W2PcEvU_Yqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.drop(columns='accountNumber',axis=1)\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAAha8P0VQMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import gc\n",
        "\n",
        "from sklearn.exceptions import NotFittedError\n",
        "\n",
        "from itertools import chain\n",
        "\n",
        "class FeatureSelector():\n",
        "    \"\"\"\n",
        "    Class for performing feature selection for machine learning or data preprocessing.\n",
        "    \n",
        "    Implements five different methods\n",
        "    \n",
        "        1. Remove columns with a missing percentage greater than a specified threshold\n",
        "        2. Remove columns with a single unique value\n",
        "        3. Remove collinear variables with a correlation greater than a specified correlation coefficient\n",
        "        4. Remove features with 0.0 feature importance from a gradient boosting machine (gbm)\n",
        "        5. Remove features that do not contribute to a specified cumulative feature importance from the gbm\n",
        "        \n",
        "    Attributes\n",
        "    --------\n",
        "    \n",
        "    record_missing : dataframe\n",
        "        Records the fraction of missing values for features with missing fraction above threshold\n",
        "    \n",
        "    record_single_unique : dataframe\n",
        "        Records the features that have a single unique value\n",
        "    \n",
        "    record_collinear : dataframe\n",
        "        Records the pairs of collinear variables with a correlation coefficient above the threshold\n",
        "    \n",
        "    record_zero_importance : dataframe\n",
        "        Records the zero importance features in the data according to the gbm\n",
        "    \n",
        "    record_low_importance : dataframe\n",
        "        Records the lowest importance features not needed to reach the threshold of cumulative importance according to the gbm\n",
        "    \n",
        "    feature_importances : dataframe\n",
        "        All the features importances from the gbm\n",
        "    \n",
        "    removal_ops : dict\n",
        "        Dictionary of removal operations and associated features for removal identified\n",
        "        \n",
        "    Notes\n",
        "    --------\n",
        "    \n",
        "        - All 5 operations can be run with the `identify_all` method.\n",
        "        - Calculating the feature importances requires labels (a supervised learning task) \n",
        "          for training the gradient boosting machine\n",
        "        - For the feature importances, the dataframe is first one-hot encoded before training the gbm.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        # Dataframes recording information about features to remove\n",
        "        self.record_missing = None\n",
        "        self.record_single_unique = None\n",
        "        self.record_collinear = None\n",
        "        self.record_zero_importance = None\n",
        "        self.record_low_importance = None\n",
        "        \n",
        "        self.feature_importances = None\n",
        "        \n",
        "        # Dictionary to hold removal operations\n",
        "        self.removal_ops = {}\n",
        "        \n",
        "        \n",
        "        \n",
        "    def identify_missing(self, data, missing_threshold):\n",
        "        \"\"\"Find the features with a fraction of missing values above `missing_threshold`\"\"\"\n",
        "        \n",
        "        self.missing_threshold = missing_threshold\n",
        "\n",
        "        # Calculate the fraction of missing in each column \n",
        "        missing_series = data.isnull().sum() / data.shape[0]\n",
        "        \n",
        "        self.missing_stats = pd.DataFrame(missing_series).rename(columns = {'index': 'feature', 0: 'missing_fraction'})\n",
        "\n",
        "        # Find the columns with a missing percentage above the threshold\n",
        "        record_missing = pd.DataFrame(missing_series[missing_series > missing_threshold]).reset_index().rename(columns = {'index': 'feature', 0: 'missing_fraction'})\n",
        "\n",
        "        to_drop = list(record_missing['feature'])\n",
        "\n",
        "        self.record_missing = record_missing\n",
        "        self.removal_ops['missing'] = to_drop\n",
        "        \n",
        "        print('%d features with greater than %0.2f missing values.\\n' % (len(self.removal_ops['missing']), self.missing_threshold))\n",
        "        \n",
        "    def identify_single_unique(self, data):\n",
        "        \"\"\"Identifies features with only a single unique value. NaNs do not count as a unique value. \"\"\"\n",
        "\n",
        "        # Calculate the unique counts in each column\n",
        "        unique_counts = data.nunique()\n",
        "\n",
        "        self.unique_stats = pd.DataFrame(unique_counts).rename(columns = {'index': 'feature', 0: 'nunique'})\n",
        "        \n",
        "        # Find the columns with only one unique count\n",
        "        record_single_unique = pd.DataFrame(unique_counts[unique_counts == 1]).reset_index().rename(columns = {'index': 'feature', 0: 'nunique'})\n",
        "\n",
        "        to_drop = list(record_single_unique['feature'])\n",
        "    \n",
        "        self.record_single_unique = record_single_unique\n",
        "        self.removal_ops['single_unique'] = to_drop\n",
        "        \n",
        "        print('%d features with a single unique value.\\n' % len(self.removal_ops['single_unique']))\n",
        "    \n",
        "    def identify_collinear(self, data, correlation_threshold):\n",
        "        \"\"\"\n",
        "        Finds collinear features based on the correlation coefficient between features. \n",
        "        For each pair of features with a correlation coefficient greather than `correlation_threshold`,\n",
        "        only one of the pair is identified for removal. \n",
        "\n",
        "        Using code adapted from: https://gist.github.com/Swarchal/e29a3a1113403710b6850590641f046c\n",
        "        \n",
        "        Parameters\n",
        "        --------\n",
        "\n",
        "        data : dataframe\n",
        "            Data observations in the rows and features in the columns\n",
        "\n",
        "        correlation_threshold : float between 0 and 1\n",
        "            Value of the Pearson correlation cofficient for identifying correlation features\n",
        "\n",
        "        \"\"\"\n",
        "        \n",
        "        self.correlation_threshold = correlation_threshold\n",
        "\n",
        "        # Calculate the correlations between every column\n",
        "        corr_matrix = data.corr()\n",
        "        \n",
        "        self.corr_matrix = corr_matrix\n",
        "    \n",
        "        # Extract the upper triangle of the correlation matrix\n",
        "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
        "        \n",
        "        # Select the features with correlations above the threshold\n",
        "        # Need to use the absolute value\n",
        "        to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
        "\n",
        "        # Dataframe to hold correlated pairs\n",
        "        record_collinear = pd.DataFrame(columns = ['drop_feature', 'corr_feature', 'corr_value'])\n",
        "\n",
        "        # Iterate through the columns to drop\n",
        "        for column in to_drop:\n",
        "\n",
        "            # Find the correlated features\n",
        "            corr_features = list(upper.index[upper[column].abs() > correlation_threshold])\n",
        "\n",
        "            # Find the correlated values\n",
        "            corr_values = list(upper[column][upper[column].abs() > correlation_threshold])\n",
        "            drop_features = [column for _ in range(len(corr_features))]    \n",
        "\n",
        "            # Record the information (need a temp df for now)\n",
        "            temp_df = pd.DataFrame.from_dict({'drop_feature': drop_features,\n",
        "                                             'corr_feature': corr_features,\n",
        "                                             'corr_value': corr_values})\n",
        "\n",
        "            # Add to dataframe\n",
        "            record_collinear = record_collinear.append(temp_df, ignore_index = True)\n",
        "\n",
        "            \n",
        "        self.record_collinear = record_collinear\n",
        "        self.removal_ops['collinear'] = to_drop\n",
        "        \n",
        "        print('%d features with a correlation greater than %0.2f.\\n' % (len(self.removal_ops['collinear']), self.correlation_threshold))\n",
        "\n",
        "    def identify_zero_importance(self, features, labels, eval_metric, task='classification', \n",
        "                                 n_iterations=10, early_stopping = True):\n",
        "        \"\"\"\n",
        "        \n",
        "        Identify the features with zero importance according to a gradient boosting machine.\n",
        "        The gbm can be trained with early stopping using a validation set to prevent overfitting. \n",
        "        The feature importances are averaged over n_iterations to reduce variance. \n",
        "        \n",
        "        Uses the LightGBM implementation (http://lightgbm.readthedocs.io/en/latest/index.html)\n",
        "\n",
        "        Parameters \n",
        "        --------\n",
        "        features : dataframe\n",
        "            Data for training the model with observations in the rows\n",
        "            and features in the columns\n",
        "\n",
        "        labels : array, shape = (1, )\n",
        "            Array of labels for training the model. These can be either binary \n",
        "            (if task is 'classification') or continuous (if task is 'regression')\n",
        "\n",
        "        eval_metric : string\n",
        "            Evaluation metric to use for the gradient boosting machine\n",
        "\n",
        "        task : string, default = 'classification'\n",
        "            The machine learning task, either 'classification' or 'regression'\n",
        "\n",
        "        n_iterations : int, default = 10\n",
        "            Number of iterations to train the gradient boosting machine\n",
        "            \n",
        "        early_stopping : boolean, default = True\n",
        "            Whether or not to use early stopping with a validation set when training\n",
        "        \n",
        "        \n",
        "        Notes\n",
        "        --------\n",
        "        \n",
        "        - Features are one-hot encoded to handle the categorical variables before training.\n",
        "        - The gbm is not optimized for any particular task and might need some hyperparameter tuning\n",
        "        - Feature importances, including zero importance features, can change across runs\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # One hot encoding\n",
        "        features = pd.get_dummies(features)\n",
        "\n",
        "        # Extract feature names\n",
        "        feature_names = list(features.columns)\n",
        "\n",
        "        # Convert to np array\n",
        "        features = np.array(features)\n",
        "        labels = np.array(labels).reshape((-1, ))\n",
        "\n",
        "        # Empty array for feature importances\n",
        "        feature_importance_values = np.zeros(len(feature_names))\n",
        "        \n",
        "        print('Training Gradient Boosting Model\\n')\n",
        "        \n",
        "        # Iterate through each fold\n",
        "        for _ in range(n_iterations):\n",
        "\n",
        "            if task == 'classification':\n",
        "                model = lgb.LGBMClassifier(n_estimators=1000, learning_rate = 0.05, verbose = -1)\n",
        "\n",
        "            elif task == 'regression':\n",
        "                model = lgb.LGBMRegressor(n_estimators=1000, learning_rate = 0.05, verbose = -1)\n",
        "\n",
        "            else:\n",
        "                raise ValueError('Task must be either \"classification\" or \"regression\"')\n",
        "                \n",
        "            # If training using early stopping need a validation set\n",
        "            if early_stopping:\n",
        "                \n",
        "                train_features, valid_features, train_labels, valid_labels = train_test_split(features, labels, test_size = 0.15)\n",
        "\n",
        "                # Train the model with early stopping\n",
        "                model.fit(train_features, train_labels, eval_metric = eval_metric,\n",
        "                          eval_set = [(valid_features, valid_labels)],\n",
        "                          early_stopping_rounds = 100, verbose = -1)\n",
        "                \n",
        "                # Clean up memory\n",
        "                gc.enable()\n",
        "                del train_features, train_labels, valid_features, valid_labels\n",
        "                gc.collect()\n",
        "                \n",
        "            else:\n",
        "                model.fit(features, labels)\n",
        "\n",
        "            # Record the feature importances\n",
        "            feature_importance_values += model.feature_importances_ / n_iterations\n",
        "\n",
        "        feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
        "\n",
        "        # Sort features according to importance\n",
        "        feature_importances = feature_importances.sort_values('importance', ascending = False).reset_index(drop = True)\n",
        "\n",
        "        # Normalize the feature importances to add up to one\n",
        "        feature_importances['normalized_importance'] = feature_importances['importance'] / feature_importances['importance'].sum()\n",
        "        feature_importances['cumulative_importance'] = np.cumsum(feature_importances['normalized_importance'])\n",
        "\n",
        "        # Extract the features with zero importance\n",
        "        record_zero_importance = feature_importances[feature_importances['importance'] == 0.0]\n",
        "        \n",
        "        to_drop = list(record_zero_importance['feature'])\n",
        "\n",
        "        self.feature_importances = feature_importances\n",
        "        self.record_zero_importance = record_zero_importance\n",
        "        self.removal_ops['zero_importance'] = to_drop\n",
        "        \n",
        "        print('\\n%d features with zero importance.\\n' % len(self.removal_ops['zero_importance']))\n",
        "    \n",
        "    def identify_low_importance(self, cumulative_importance):\n",
        "        \"\"\"\n",
        "        Finds the lowest importance features not needed to account for `cumulative_importance` \n",
        "        of the feature importance from the gradient boosting machine. As an example, if cumulative\n",
        "        importance is set to 0.95, this will retain only the most important features needed to \n",
        "        reach 95% of the total feature importance. The identified features are those not needed.\n",
        "\n",
        "        Parameters\n",
        "        --------\n",
        "        cumulative_importance : float between 0 and 1\n",
        "            The fraction of cumulative importance to account for \n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.cumulative_importance = cumulative_importance\n",
        "        \n",
        "        # The feature importances need to be calculated before running\n",
        "        if self.feature_importances is None:\n",
        "            raise NotFittedError('Feature importances have not yet been determined. Call the `identify_zero_importance` method` first.')\n",
        "            \n",
        "        # Make sure most important features are on top\n",
        "        self.feature_importances = self.feature_importances.sort_values('cumulative_importance')\n",
        "\n",
        "        # Identify the features not needed to reach the cumulative_importance\n",
        "        record_low_importance = self.feature_importances[self.feature_importances['cumulative_importance'] > cumulative_importance]\n",
        "\n",
        "        to_drop = list(record_low_importance['feature'])\n",
        "\n",
        "        self.record_low_importance = record_low_importance\n",
        "        self.removal_ops['low_importance'] = to_drop\n",
        "    \n",
        "        print('%d features that do not contribute to cumulative importance of %0.2f.\\n' % (len(self.removal_ops['low_importance']), self.cumulative_importance))\n",
        "        \n",
        "    def identify_all(self, features, labels, selection_params):\n",
        "        \"\"\"\n",
        "        Use all five of the methods to identify features to remove.\n",
        "        \n",
        "        Parameters\n",
        "        --------\n",
        "        \n",
        "        features : dataframe\n",
        "            Data for training the model with observations in the rows\n",
        "            and features in the columns\n",
        "\n",
        "        labels : array, shape = (1, )\n",
        "            Array of labels for training the model. These can be either binary \n",
        "            (if task is 'classification') or continuous (if task is 'regression')\n",
        "            \n",
        "        selection_params : dict\n",
        "           Parameters to use in the five feature selection methhods.\n",
        "           Params must contain the keys ['missing_threshold', 'correlation_threshold', 'eval_metric', 'task', 'cumulative_importance']\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        # Check for all required parameters\n",
        "        for param in ['missing_threshold', 'correlation_threshold', 'eval_metric', 'task', 'cumulative_importance']:\n",
        "            if param not in selection_params.keys():\n",
        "                raise ValueError('%s is a required parameter for this method' % param)\n",
        "        \n",
        "        # Implement each of the five methods\n",
        "        self.identify_missing(features, selection_params['missing_threshold'])\n",
        "        self.identify_single_unique(features)\n",
        "        self.identify_collinear(features, selection_params['correlation_threshold'])\n",
        "        self.identify_zero_importance(features, labels, selection_params['eval_metric'], selection_params['task'])\n",
        "        self.identify_low_importance(selection_params['cumulative_importance'])\n",
        "        \n",
        "        # Find the number of features identified to drop\n",
        "        self.n_identified = len(set(list(chain(*list(self.removal_ops.values())))))\n",
        "        print('%d total features out of %d identified for removal.\\n' % (self.n_identified, pd.get_dummies(features).shape[1]))\n",
        "        \n",
        "    def check_identified(self):\n",
        "        \"\"\"Check the identified features before removal. Returns a set of the unique features identified.\"\"\"\n",
        "        \n",
        "        all_identified = set(list(chain(*list(self.removal_ops.values()))))\n",
        "        print('%d features identified for removal' % len(all_identified))\n",
        "        \n",
        "        return all_identified\n",
        "        \n",
        "    \n",
        "    def remove(self, data, methods):\n",
        "        \"\"\"\n",
        "        Remove the features from the data according to the specified methods.\n",
        "        \n",
        "        Parameters\n",
        "        --------\n",
        "            data : dataframe\n",
        "                Dataframe with features to remove\n",
        "            methods : 'all' or list of methods\n",
        "                If methods == 'all', any methods that have identified features will be used\n",
        "                Otherwise, only the specified methods will be used.\n",
        "                Can be one of ['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance']\n",
        "                \n",
        "        Return\n",
        "        --------\n",
        "            data : dataframe\n",
        "                Dataframe with identified features removed\n",
        "                \n",
        "        \n",
        "        Notes \n",
        "        --------\n",
        "            - This first one-hot encodes the categorical variables in accordance with the gradient boosting machine.\n",
        "            - Check the features that will be removed before transforming data!\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        features_to_drop = []\n",
        "        \n",
        "        data = pd.get_dummies(data)\n",
        "        \n",
        "        if methods == 'all':\n",
        "            \n",
        "            print('{} methods have been run'.format(list(self.removal_ops.keys())))\n",
        "            \n",
        "            # Find the unique features to drop\n",
        "            features_to_drop = set(list(chain(*list(self.removal_ops.values()))))\n",
        "            \n",
        "        else:\n",
        "            # Iterate through the specified methods\n",
        "            for method in methods:\n",
        "                # Check to make sure the method has been run\n",
        "                if method not in self.removal_ops.keys():\n",
        "                    raise NotFittedError('%s method has not been run' % method)\n",
        "                    \n",
        "                # Append the features identified for removal\n",
        "                else:\n",
        "                    features_to_drop.append(self.removal_ops[method])\n",
        "        \n",
        "            # Find the unique features to drop\n",
        "            features_to_drop = set(list(chain(*features_to_drop)))\n",
        "            \n",
        "        # Remove the features and return the data\n",
        "        data = data.drop(columns = features_to_drop)\n",
        "        self.removed_features = features_to_drop\n",
        "        \n",
        "        print('Removed %d features' % len(features_to_drop))\n",
        "        return data\n",
        "    \n",
        "    def plot_missing(self):\n",
        "        \"\"\"Histogram of missing fraction in each feature\"\"\"\n",
        "        if self.record_missing is None:\n",
        "            raise NotImplementedError(\"Missing values have not been calculated. Run `identify_missing`\")\n",
        "        \n",
        "        self.reset_plot()\n",
        "        self.missing_stats.plot.hist(color = 'red', edgecolor = 'k', figsize = (6, 4), fontsize = 14)\n",
        "        plt.ylabel('Frequency', size = 18)\n",
        "        plt.xlabel('Missing Fraction', size = 18); plt.title('Missing Fraction Histogram', size = 18);\n",
        "        \n",
        "    \n",
        "    def plot_unique(self):\n",
        "        \"\"\"Histogram of number of unique values in each feature\"\"\"\n",
        "        if self.record_single_unique is None:\n",
        "            raise NotImplementedError('Unique values have not been calculated. Run `identify_single_unique`')\n",
        "        \n",
        "        self.reset_plot()\n",
        "        self.unique_stats.plot.hist(edgecolor = 'k', figsize = (6, 4), fontsize = 14)\n",
        "        plt.ylabel('Frequency', size = 18)\n",
        "        plt.xlabel('Unique Values', size = 18); plt.title('Unique Values Histogram', size = 18);\n",
        "        \n",
        "    \n",
        "    def plot_collinear(self):\n",
        "        \"\"\"\n",
        "        Heatmap of the features with correlations above the correlated threshold in the data.\n",
        "        \n",
        "        Notes\n",
        "        --------\n",
        "            - Not all of the plotted correlations are above the threshold because this plots\n",
        "            all the variables that have been idenfitied as having even one correlation above the threshold\n",
        "            - The features on the x-axis are those that will be removed. The features on the y-axis\n",
        "            are the correlated feature with those on the x-axis\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        if self.record_collinear is None:\n",
        "            raise NotImplementedError('Collinear features have not been idenfitied. Run `identify_collinear`.')\n",
        "        \n",
        "        # Identify the correlations that were above the threshold\n",
        "        corr_matrix_plot = self.corr_matrix.loc[list(set(self.record_collinear['corr_feature'])), \n",
        "                                                list(set(self.record_collinear['drop_feature']))]\n",
        "\n",
        "        # Set up the matplotlib figure\n",
        "        f, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "        # Generate a custom diverging colormap\n",
        "        cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "        # Draw the heatmap with the mask and correct aspect ratio\n",
        "        sns.heatmap(corr_matrix_plot, cmap=cmap, center=0,\n",
        "                    linewidths=.25, cbar_kws={\"shrink\": 0.6})\n",
        "\n",
        "        ax.set_yticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[0]))])\n",
        "        ax.set_yticklabels(list(corr_matrix_plot.index), size = int(160 / corr_matrix_plot.shape[0]));\n",
        "\n",
        "        ax.set_xticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[1]))])\n",
        "        ax.set_xticklabels(list(corr_matrix_plot.columns), size = int(160 / corr_matrix_plot.shape[1]));\n",
        "        \n",
        "        plt.xlabel('Features to Remove', size = 8); plt.ylabel('Correlated Feature', size = 8)\n",
        "        plt.title(\"Correlations Above Threshold\", size = 14)\n",
        "        \n",
        "    def plot_feature_importances(self, threshold = None):\n",
        "        \"\"\"\n",
        "        Plots 15 most important features and the cumulative importance of features.\n",
        "        If `threshold` is provided, prints the number of features needed to reach `threshold` cumulative importance.\n",
        "\n",
        "        Parameters\n",
        "        --------\n",
        "        threshold : float, between 0 and 1 default = None\n",
        "            Threshold for printing information about cumulative importances\n",
        "\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.record_zero_importance is None:\n",
        "            raise NotImplementedError('Feature importances have not been determined. Run `idenfity_zero_importance`')\n",
        "\n",
        "        self.reset_plot()\n",
        "        \n",
        "        # Make a horizontal bar chart of feature importances\n",
        "        plt.figure(figsize = (10, 6))\n",
        "        ax = plt.subplot()\n",
        "\n",
        "        # Need to reverse the index to plot most important on top\n",
        "        ax.barh(list(reversed(list(self.feature_importances.index[:15]))), \n",
        "                self.feature_importances['normalized_importance'].head(15), \n",
        "                align = 'center', edgecolor = 'k')\n",
        "\n",
        "        # Set the yticks and labels\n",
        "        ax.set_yticks(list(reversed(list(self.feature_importances.index[:15]))))\n",
        "        ax.set_yticklabels(self.feature_importances['feature'].head(15), size = 12)\n",
        "\n",
        "        # Plot labeling\n",
        "        plt.xlabel('Normalized Importance', size = 16); plt.title('Feature Importances', size = 18)\n",
        "        plt.show()\n",
        "\n",
        "        # Cumulative importance plot\n",
        "        plt.figure(figsize = (6, 4))\n",
        "        plt.plot(list(range(1, len(self.feature_importances) + 1)), self.feature_importances['cumulative_importance'], 'r-')\n",
        "        plt.xlabel('Number of Features', size = 14); plt.ylabel('Cumulative Importance', size = 14); \n",
        "        plt.title('Cumulative Feature Importance', size = 16);\n",
        "\n",
        "        if threshold:\n",
        "\n",
        "            # Index of minimum number of features needed for cumulative importance threshold\n",
        "            importance_index = np.min(np.where(self.feature_importances['cumulative_importance'] > threshold))\n",
        "            plt.vlines(x = importance_index + 1, ymin = 0, ymax = 1, linestyles='--', colors = 'blue')\n",
        "            plt.show()\n",
        "\n",
        "            print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))\n",
        "\n",
        "        \n",
        "    def reset_plot(self):\n",
        "        plt.rcParams = plt.rcParamsDefault"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVs81CFaVhrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fs = FeatureSelector()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgJRB-0JVlaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fs.identify_missing(data, 0.9)\n",
        "fs.plot_missing()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReCbFA1JVp4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fs.identify_single_unique(data)\n",
        "fs.plot_unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEp58HSSVup3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fs.identify_collinear(data, 0.99)\n",
        "fs.plot_collinear()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3mrCWCiV0W0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fs.identify_collinear(data, 0.90)\n",
        "fs.plot_collinear()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi0QjvGcV8qY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_labels = data['isFraud']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWE9NH2lWEkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fs.identify_zero_importance(data, data_labels, eval_metric='auc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm9LtK9HWS-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fs.plot_feature_importances(threshold = 0.99)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSSaUdmpaaCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fs.identify_low_importance(0.99)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBPwpgjztkC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fs.identify_all(data, data_labels, {'missing_threshold': 0.9, 'correlation_threshold': 0.95, 'eval_metric': 'auc',\n",
        "                                      'task': 'classification', 'cumulative_importance': 0.95})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DetrawttkKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_identified = fs.check_identified()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmtI6wbLtkNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(features_identified)[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgm0cJwgtxo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_removed = fs.remove(data, methods = 'all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3U9zwsPtxsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_removed.to_json(r'path')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IVxh4kGt6dC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4XN8d3qt6fO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}